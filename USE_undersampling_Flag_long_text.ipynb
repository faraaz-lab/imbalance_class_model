import pandas as pd
import numpy as np
import traceback
#Data Visualization
import matplotlib.pyplot as plt

#Text Color
from termcolor import colored

#Train Test Split
from sklearn.model_selection import train_test_split

#Model Evaluation
from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score
# from mlxtend.plotting import plot_confusion_matrix

#Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.utils import plot_model
from tensorflow_hub import KerasLayer

from sklearn.model_selection import train_test_split
from unidecode import unidecode
import re

from  tensorflow.keras  import  backend  as  K

import tensorflow_hub as hub


#some additional metrics, to-do: definition of true positives according to serious=yes o nly
def mcor(y_true, y_pred):
    try:
        #matthews_correlation
        y_pred_pos  =  K.round(K.clip(y_pred,  0,  1)) 
        y_pred_neg = 1 - y_pred_pos
        y_pos  =  K.round(K.clip(y_true,  0,  1)) 
        y_neg = 1 - y_pos
        tp  =  K.sum(y_pos  *  y_pred_pos) 
        tn  =  K.sum(y_neg  *  y_pred_neg) 
        fp  =  K.sum(y_neg  *  y_pred_pos) 
        fn  =  K.sum(y_pos  *  y_pred_neg) 
        numerator = (tp * tn - fp * fn)
        denominator  =  K.sqrt((tp  +  fp)  *  (tp  +  fn)  *  (tn  +  fp)  *  (tn  +  fn))
        return  numerator  /  (denominator  +  K.epsilon())
    except:
        traceback.print_exc()
        

def recall_dl(y_true, y_pred):
    try:
        y_pred_pos  =  K.round(K.clip(y_pred,  0,  1)) 
        y_pred_neg = 1 - y_pred_pos
        y_pos  =  K.round(K.clip(y_true,  0,  1)) 
        y_neg = 1 - y_pos
        tp  =  K.sum(y_pos  *  y_pred_pos) 
        tn  =  K.sum(y_neg  *  y_pred_neg) 
        fp  =  K.sum(y_neg  *  y_pred_pos) 
        fn  =  K.sum(y_pos  *  y_pred_neg) 
        return tp / (tp+fn)
    except:
        traceback.print_exc()
        
def precision_dl(y_true, y_pred):
    try:
        y_pred_pos  =  K.round(K.clip(y_pred,  0,  1)) 
        y_pred_neg = 1 - y_pred_pos
        y_pos  =  K.round(K.clip(y_true,  0,  1)) 
        y_neg = 1 - y_pos
        tp  =  K.sum(y_pos  *  y_pred_pos) 
        tn  =  K.sum(y_neg  *  y_pred_neg) 
        fp  =  K.sum(y_neg  *  y_pred_pos) 
        fn  =  K.sum(y_pos  *  y_pred_neg) 
        return tp / (tp+fp)
    except:
        traceback.print_exc()

def f1_dl(y_true,y_pred):
    try:
        return  2*((precision_dl(y_true,y_pred)*recall_dl(y_true,y_pred))/(precision_dl(y_true,y_pred)+recall_dl(y_true,y_pred)))
    except:
        traceback.print_exc()

# def single_class_precision(interesting_class_id):
#     def sc_precision(y_true, y_pred):
#         try:
#             class_id_true  =  K.argmax(y_true,  axis=-1) 
#             class_id_preds  =  K.argmax(y_pred,  axis=-1)
#             # Replace class_id_preds with class_id_true for recall here
#             accuracy_mask  =  K.cast(K.equal(class_id_preds,  interesting_class_id),  'int32') 
#             class_acc_tensor  =  K.cast(K.equal(class_id_true,  class_id_preds),  'int32')  *  acc
#             uracy_mask
#             class_acc  =  K.sum(class_acc_tensor)  /  K.maximum(K.sum(accuracy_mask),  1)
#             return class_acc
#         except:
#             traceback.print_exc()
#     return sc_precision


def recall_m(y_true, y_pred):
    try:
        #	true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        true_positives= tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred)) 
        possible_positives  =  K.sum(K.round(K.clip(y_true,  0,  1)))
        recall  =  true_positives  /  (possible_positives  +  K.epsilon())
        return recall
    except:
        traceback.print_exc()

def precision_m(y_true, y_pred):
    try:
        true_positives  =  K.sum(K.round(K.clip(y_true  *  y_pred,  0,  1))) 
        predicted_positives  =  K.sum(K.round(K.clip(y_pred,  0,  1)))
        precision  =  true_positives  /  (predicted_positives  +  K.epsilon())
        return precision
    except:
        traceback.print_exc()

def  f1_m(y_true,  y_pred):
    try:
        precision = precision_m(y_true, y_pred) 
        recall = recall_m(y_true, y_pred)
        return  2*((precision*recall)/(precision+recall+K.epsilon()))
    except:
        traceback.print_exc()
def f1(y_true, y_pred):
    y_pred = K.round(y_pred)
    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)
    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)
    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)
    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)

    p = tp / (tp + fp + K.epsilon())
    r = tp / (tp + fn + K.epsilon())

    f1 = 2*p*r / (p+r+K.epsilon())
#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)
    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)
    return K.mean(f1)
def recall(y_true, y_pred):
    y_pred = K.round(y_pred)
    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)
    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)
    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)
    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)

#     p = tp / (tp + fp + K.epsilon())
    r = tp / (tp + fn + K.epsilon())

#     f1 = 2*p*r / (p+r+K.epsilon())
# #     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)
#     f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)
    return K.mean(r)
def precision(y_true, y_pred):
    y_pred = K.round(y_pred)
    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)
    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)
    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)
    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)

    p = tp / (tp + fp + K.epsilon())
#     r = tp / (tp + fn + K.epsilon())

#     f1 = 2*p*r / (p+r+K.epsilon())
# #     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)
#     f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)
    return K.mean(p)

def clean_text(raw_text):
    try:
        if type(raw_text)==None:
            return 'NAN'
        text=unidecode(raw_text)
        text=str(text).lower() #Normalization
        text=re.sub(r"(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", text) # Removing Unicode Characters
        text=str(text).replace("\n","").replace("\r","").replace("\t","").replace(' \n ',' ')
        text=str(text).strip()

    #         text = re.sub("https?://.*[\t\r\n]*", "", text)
        return text
    except:
        print(raw_text)
        traceback.print_exc()
        return 'NAN'

random_state=802
identifier="21Feb2023_USE_undersampling_long_text"
df = pd.read_pickle("input_output_pickle/long_text_source_type.pkl")
# df=df[:1000]
print("df.SOURCE_TYPE.value_counts()",df.SOURCE_TYPE.value_counts())    

df=df.dropna()
print(df.shape)
print(df.columns)
# exit()

df['text']=df['long_text'].apply(clean_text)
print(df[df['text']=='NAN'].shape)


print(df.shape)

df=df.reset_index()

to_predict_flagnames=['<flagname>',
       ]

for to_predict_flagname in to_predict_flagnames:
    no_variants=['no','nO','No','NO']
    for no_variant in no_variants:
        df.loc[df[to_predict_flagname] == no_variant, to_predict_flagname] = 0
    yes_variants=['yes' ,'yeS' ,'yEs' ,'yES' ,'Yes' ,'YeS' ,'YEs' ,'YES']
    for yes_variant in yes_variants:
        df.loc[df[to_predict_flagname] == yes_variant, to_predict_flagname] = 1


print(df.shape)

df=df.reset_index()

import sklearn
def train_use_model(to_predict_flagname,df,feature_to_train,identifier):

    print("***************to_predict_flagname",to_predict_flagname)
    print(df[to_predict_flagname].value_counts())

    negative_indices = df[df[to_predict_flagname] == 0].index #33898 => picking random 
    sample_size = sum(df[to_predict_flagname] == 1) #7063
    random_indices = np.random.choice(negative_indices, sample_size, replace=False)
    print("negative_indices",len(negative_indices),"sample_size",sample_size,"random_indices",len(random_indices))

    negative_df=df.loc[df.index[random_indices]]
    positive_df=df[df[to_predict_flagname]==1]
    print(negative_df.shape,positive_df.shape)

    input_df=pd.concat([negative_df,positive_df])



    all_train = input_df.sample(frac=0.80, random_state=random_state)
    all_test = input_df.drop(all_train.index)

    train_df =all_train.copy()
    test_df  =all_test.copy()

    # all_train=df[df.train_valid=="train"]
    # all_test=df[df.train_valid=="valid"]
    print("all_train",all_train.shape)
    print("all_test",all_test.shape)

    print("Training value counts",all_train[to_predict_flagname].value_counts())
    print("Testing value counts",all_test[to_predict_flagname].value_counts())

    train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=random_state)

    train_X = train_df[feature_to_train].values
    val_X = val_df[feature_to_train].values

    train_y = train_df[to_predict_flagname].values.astype(int)
    val_y = val_df[to_predict_flagname].values.astype(int)

    print(train_X.shape,val_X.shape,train_y.shape,val_y.shape)


    model = tf.keras.models.Sequential()
    model.add(hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder/4", 
                            input_shape=[], 
                            dtype=tf.string, 
                            trainable=True))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

    print(model.summary())

    model.compile(optimizer='adam', 
                  loss='binary_crossentropy', 
                  metrics=['accuracy',f1,precision,recall])



    history=model.fit(train_X, 
              train_y, 
              epochs=10, 
              validation_data=(val_X, val_y)
                     )



    fname=identifier+'model_universal_sentence_encoder_v1.h5'
    model.save(fname)


    test_X = test_df[feature_to_train].values
    test_y = test_df[to_predict_flagname].values.astype(int)

    y_hat = model.predict(test_X)

    test_df["predicted_01"]=np.where(y_hat < 0.5, 0, 1)


    print(test_df.predicted_01.value_counts())
    print(test_df[to_predict_flagname].value_counts())
    print("accuracy_score",sklearn.metrics.accuracy_score(test_df[to_predict_flagname].values.astype(int), test_df.predicted_01.values.astype(int)))

    conf = sklearn.metrics.confusion_matrix(test_df[to_predict_flagname].values.astype(int), test_df.predicted_01.values.astype(int))
    print(conf)
    tn, fp, fn, tp=sklearn.metrics.confusion_matrix(test_df[to_predict_flagname].values.astype(int), test_df.predicted_01.values.astype(int)).ravel()
    # tn, fp, fn, tp=confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))
    # evaluation_metrics(tn, fp, fn, tp)

    print(sklearn.metrics.classification_report(test_df[to_predict_flagname].values.astype(int), test_df.predicted_01.values.astype(int)))


train_use_model('<flagname>',df,'text','<flagname>'+identifier)
